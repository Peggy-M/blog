# 跨分片时间范围查询高延迟解决方案

针对分片环境下按时间范围查询的高延迟问题，以下是经过实战验证的多层次优化方案：

## 一、架构层优化（根本解决）

### 1. 双重分片策略
**组合分片键设计**：
```sql
分片键 = 时间分片（每小时一个分片） + 用户ID哈希分片
```
- 最近1小时数据：`current_hour_shard + user_hash`
- 历史数据：`archive_${year}${month} + user_hash`

**优势**：
- 查询最近数据只需访问固定数量的分片（如用户哈希分片数为8，则最多查8个分片）
- 自动冷热分离，历史数据可压缩存储

### 2. 实时聚合层
```python
# Flink实时聚合示例
orders_stream.key_by(lambda x: x["user_id"]) \
    .window(TumblingEventTimeWindows.of(Time.hours(1))) \
    .aggregate(OrderAggregator()) \
    .add_sink(RedisSink())
```
**输出**：
- Redis ZSET存储每个用户的最新订单：`user:123:recent_orders`
- 直接通过用户维度获取，完全避免跨分片查询

## 二、查询层优化（快速见效）

### 1. 并行查询+智能归并
```java
// 使用CompletableFuture并行查询（带超时控制）
List<Callable<List<Order>>> tasks = shards.stream()
    .map(shard -> (Callable<List<Order>>) () -> 
        shard.queryRecentOrders(userId, Duration.ofHours(1)))
    .collect(Collectors.toList());

List<Future<List<Order>>> futures = executor.invokeAll(tasks, 300, TimeUnit.MILLISECONDS);

// 结果归并时按业务优先级处理
List<Order> results = futures.stream()
    .filter(Future::isDone)
    .flatMap(f -> {
        try {
            return f.get().stream();
        } catch (Exception e) {
            return Stream.empty(); // 降级处理
        }
    })
    .sorted(comparing(Order::getCreateTime).reversed())
    .limit(1000) // 结果截断
    .collect(Collectors.toList());
```

### 2. 分片路由优化
**智能路由表**：
```json
{
  "shard_rules": {
    "recent_1h": ["shard_1", "shard_3"], // 最近数据只存在于特定分片
    "default": ["shard_1", "shard_2", "shard_3", "shard_4"]
  }
}
```
- 通过元数据服务维护数据分布
- 查询时先获取目标分片列表，减少不必要的分片访问

## 三、缓存层优化（性能倍增）

### 1. 多级缓存策略
**缓存架构**：
```
请求 → 本地缓存（Caffeine） → 分布式缓存（Redis） → 分片DB
```

**缓存键设计**：
```java
String cacheKey = String.format("recent_orders:%s:%d", 
    userId, 
    Instant.now().getEpochSecond() / 3600); // 每小时一个key
```

### 2. 增量缓存预热
```python
# 定时任务预加载热点数据
def preheat_cache():
    hot_users = get_frequent_users() # 获取最近活跃用户
    for user in hot_users:
        orders = query_recent_orders(user.id)
        redis.zadd(f"user:{user.id}:orders", 
            {order.id: order.timestamp for order in orders})
```

## 四、存储层优化（长效方案）

### 1. 列式存储冷数据
```sql
-- 使用ClickHouse存储历史订单
CREATE TABLE orders_archive (
    user_id UInt64,
    order_time DateTime64(3),
    -- 其他字段...
    INDEX idx_time order_time TYPE minmax GRANULARITY 3
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(order_time)
ORDER BY (user_id, order_time);
```
- 最近数据：MySQL分片
- 历史数据：ClickHouse（查询性能提升10倍+）

### 2. 分布式索引服务
**Elasticsearch索引设计**：
```json
PUT /orders
{
  "mappings": {
    "properties": {
      "user_id": { "type": "keyword" },
      "create_time": { "type": "date" },
      "shard_id": { "type": "keyword" } // 包含原始分片信息
    }
  }
}
```
查询流程：
1. 先通过ES定位满足条件的记录及所在分片
2. 精准查询目标分片获取完整数据

## 五、监控与降级

### 1. 关键监控指标
| 指标名称                  | 报警阈值 | 监控手段          |
| ------------------------- | -------- | ----------------- |
| cross_shard_query_latency | >300ms   | Prometheus+Granfa |
| partial_result_ratio      | >5%      | 日志分析          |
| cache_hit_rate            | <90%     | Redis监控         |

### 2. 降级策略
```java
// 查询降级逻辑
public List<Order> getRecentOrdersFallback(User user, Duration duration) {
    // 1. 尝试只查主分片
    List<Order> orders = primaryShard.queryRecentOrders(user, duration);
    if (orders.size() > MIN_RESULTS) {
        return orders;
    }
    
    // 2. 降级查缓存
    orders = cacheService.getRecentOrders(user.getId());
    if (!orders.isEmpty()) {
        return orders.stream()
            .filter(o -> isWithinDuration(o, duration))
            .collect(Collectors.toList());
    }
    
    // 3. 返回兜底数据
    return Collections.emptyList();
}
```

## 六、方案选型建议

根据业务场景选择组合方案：

1. **电商交易系统**：
   - `双重分片 + 并行查询 + Redis缓存`
   - P99延迟可控制在200ms内

2. **金融账户系统**：
   - `Flink实时聚合 + 多级缓存`
   - 保证强一致性，延迟<100ms

3. **物流追踪系统**：
   - `ES索引 + ClickHouse冷存储`
   - 支持海量历史数据查询

**实施路线图**：
1. 先实现并行查询和缓存（1周）
2. 增加分片路由优化（2周）
3. 逐步引入实时聚合（1个月）
4. 历史数据迁移到列式存储（持续进行）